#### This is an example of structured-streaming with latest Spark v2.2.0.
##### A spark job reads from Kafka topic, manipulates data as datasets/dataframes and writes to Cassandra.


#### Usage:

1. Inside `setup` directory, run `docker-compose up -d` to launch instances of `zookeeper`, `kafka` and `cassandra`

1. Wait for a few seconds and then run `docker ps` to make sure all the three services are running.

1. Set up a Twitter application at [Twitter Apps](https://apps.twitter.com). You will need a Twitter account for this if you have not set one up already.

1. Using the values generated in the previous step, create a file at `producer/src/main/resources/twitter4j.properties` that looks like this:
    ```
    debug=false
    oauth.consumerKey=<key>
    oauth.consumerSecret=<secret>
    oauth.accessToken=<token>
    oauth.accessTokenSecret=<tokenSecret>
    ```

1. You may run the producer in one of three ways:
    1. using `sbt 'project producer' clean compile run` in a console. 
    1. Run the producer application in your IDE through the main entry point 
    1. Use `sbt assembly` to generate "fat jars" that can be launched `java -jar` style.
    
    This app will listen on topic (check Main.scala) and writes it to Cassandra.

1. Finally check if the data has been published in cassandra.
  * Go to cqlsh `docker exec -it cas_01_test cqlsh localhost`
  * And then run `select * from my_keyspace.test_table  ;`
  
#### Design choices

* This app is somewhat advanced for introductory users. The goal of the SKC class is to quickly move beyond the mechanics of collecting content from Twitter and moving it through Kafka, Cassandra and Spark. Since our interest is in what we can do with topics and Spark, I've extracted and reduced the code that as much as it could be so it could be "mindfully *abstracted*". So don't get *distracted* by the "Scalaisms" and understanding each line. Do that over six months as your skills with Scala improves. 

* One of the key pieces I wanted to illustrate with this code is the generation of objects from Avro schemas. In this style of programming, we let the build generate the `case class` for the types we wish to serialize and let the compiler warn us if our code has not kept up with the changes we have made. The alternative is to manually maintain the pairing between the Avro schemas internal types. Over time, this is more error prone.

* As mentioned in class, generating the schema metadata to the stream provides backward compatibility to objects serialized with older versions of the schema, but it does not eliminate the rules that must be followed. It's beyond the scope of the class to go deep into those rules, but they are not difficult. 
  
#### Where to go from here?

This class is less focused on wiring all these elements together. There are a lot of specific details that are not relevant to the course (for instance, SBT usage).

Instead, the app you have is a base to start doing interesting things with the Tweets that your machine can collect. What kinds of interesting things? That's what day 4 is all about! 

Some ideas:

* The current system does not depend at all on who your account follows, rather it is using a search for coffee and Starbucks related terms. Try changing these search keys for a very easy starter project.
* Instead of running all the searches you can imagine into a single Kafka topic and Cassandra table, update the code so it can use separate topics and tables for this data.
* The initial Avro schema is very simple: An actual Tweet has hundreds of fields in JSON. Look for fields that might be interesting and add them to the schema in a way that the old Tweets are still accessible using the power provided by Avro.
* Now that we can collect interesting streams of information, do something with Spark Streaming to start making inferences from the data. There are dozens of directions one can go with this, including starting to look at Tweet contents for actionable events.
* The inferencing of data to derive semantics can create what's called "derived events". Create Tweet streams that can use derived events in a manner that the system can summarize what might be happening in the world. 
    * Train MLlib using tweets for specific searches to generate the derived events
    * Post these derived events in a new Kafka topic and Cassandra table    
* Create additional listeners that can update your Zeppelin notebook with live graphics in a dashboard style. The listeners might be coming from direct data in tweets including
    * number of tweets per minute over the course of several hours
    * graphs of types and time of derived events generated by MLlib
* Use the functionality of GraphX to better show information about users:
    * From users that show up in searches such as for coffee, who do they know?
    * these user graphs form clusters, what are the predominant discussions of users in those clusters?
    * can you find influencers in the clusters?
    
There are **MANY** ways to work with this data with limited changes to the system you are starting with. Even though, these changes will take time to implement. Instead of trying to do a big bunch of these by yourself, consider the following teamwork ideas:
    * Use pair programming to speed development on more difficult modules
    * Use parallel development for simpler modules that might depend on each other.
The key point between these two is Kafka topics **are the ideal handoff method between modules that depend on each other!**
    

#### Credits:

This application has inspiration from the following apps:
* Ansrivas' [Spark Structured Streaming](https://github.com/ansrivas/spark-structured-streaming/tree/avro-example/setup) forms the original consumer-side code and the Docker server.
* Knoldus' [Kafka Tweet Producer](https://github.com/knoldus/kafka-tweet-producer.g8/blob/master/build.sbt) forms the original producer-side code.

From these apps, the builds were unified, a common library extracted and SBT assemblies generated, then Avro schema for tweets was generated and tested.